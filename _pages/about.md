---
permalink: /
title: "Ji-Ung Lee"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

Hi there! I am Ji-Ung, a postdoc at the research training group [Neuroexplicit Models](https://www.neuroexplicit.org/) at the University of Saarland (Germany). My research revolves around efficient model training in natural language processing (NLP). This usually involves methods such as active learning that can handle low-resource scenarios with users who can provide the labels for queried instances. I am also interested in (human) language learning, so the evaluation of my methods often happens within the context of automated exercise generation and assessment. Finally, I am a big fan of user studies, having devised and conducted various evaluation studies involving citizen scientists. If you are interested in these research topics feel free to drop me a message! 

News
======

New Preprint [(paper)](https://arxiv.org/abs/2502.12992)
------
B-cos LM: Efficiently Transforming Pre-trained Language Models for Improved Explainability. _Yifan Wang, Sukrut Rao, Ji-Ung Lee, Mayank Jobanputra, Vera Demberg_. 2025. 

> **Abstract:** Post-hoc explanation methods for black-box models often struggle with faithfulness and human interpretability due to the lack of explainability in current neural models. Meanwhile, B-cos networks have been introduced to improve model explainability through architectural and computational adaptations, but their application has so far been limited to computer vision models and their associated training pipelines. In this work, we introduce B-cos LMs, i.e., B-cos networks empowered for NLP tasks. Our approach directly transforms pre-trained language models into B-cos LMs by combining B-cos conversion and task fine-tuning, improving efficiency compared to previous B-cos methods. Our automatic and human evaluation results demonstrate that B-cos LMs produce more faithful and human interpretable explanations than post hoc methods, while maintaining task performance comparable to conventional fine-tuning. Our in-depth analysis explores how B-cos LMs differ from conventionally fine-tuned models in their learning processes and explanation patterns. Finally, we provide practical guidelines for effectively building B-cos LMs based on our findings. 

Full-day workshop on research data management [(linkedin)](https://www.linkedin.com/feed/update/urn:li:activity:7299739481656422402/)
------
On February 17, I gave a workshop on research data management at our RTG. The workshop covered various aspects around the collection, processing, and storage of research data, and also provided insights on how to conduct reproducible research, especially when working with neural models. The workshop ended with a fruitful discussion where we decided upon practical guidelines for our RTG. 


Activities
======

Workshop Organization
------
Interactive learning for natural language processing (InterNLP). 

* [InterNLP 2022](https://internlp.github.io/2022/index.html) (co-located with NeurIPS 2022)
* [InterNLP 2021](https://sites.google.com/view/internlp2021/home) (co-located with ACL 2021)

Community Engagement
------
Since 2019, I am a regular reviewer at major NLP and machine learning conferences (please check my [cv](https://ji-ung-lee.github.io/files/github-cv.pdf) for details). I have received outstanding reviewer awards, most notably at ACL 2020, EACL 2021, and ARR. I also review at various workshops such as [BEA](https://sig-edu.org/bea/).

Talks
======
* Invited talk: Facets of efficiency in NLP ([youtube](https://www.youtube.com/watch?v=Q6vs2nc2-HA)). KUIS AI, KoÂ¸c University. September, 2022.
* Spotlight talk: Investigating rational activation functions to train Transformer models. Dagstuhl Seminar on Efficient and Equitable Natural Language Processing in the Age of Deep Learning ([webpage](https://www.dagstuhl.de/en/seminars/seminar-calendar/seminar-details/22232)). June, 2022.

Teaching
======
* Winter 2024/25: Guest lecture on ethics in NLP at the University of Saarland
* Winter 2019/20 and Winter 2020/21: Project coordinator of 50+ student software projects ([Bachelorpraktikum](https://www.informatik.tu-darmstadt.de/studium_fb20/im_studium/studiengaenge_liste/bachelor_praktikum.de.jsp))
* Summer 2018: Text analytics [course](https://www.tucan.tu-darmstadt.de/scripts/mgrqispi.dll?APPNAME=CampusNet&PRGNAME=COURSEDETAILS&ARGUMENTS=-N000000000000002,-N000608,-N0,-N365583171866414,-N365583171891415,-N0,-N0,-N0) on active learning for undergraduate and graduate students
* Student thesis supervision: During my PhD time, I had the pleasure to supervise 9 motivated students on their B.Sc. and M.Sc. thesis. 
